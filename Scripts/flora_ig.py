# -*- coding: utf-8 -*-
"""FloRa IG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b7eelUx1vKNFnOe3YVm2HkFBBhefrjsC
"""

import pandas as pd

df=pd.read_csv('/content/drive/MyDrive/SDN/df_final')

df

import math

def calculate_entropy(data):
    """Calculate entropy of a dataset based on class labels."""
    class_counts = {}
    for entry in data:
        label = entry[-1]
        if label not in class_counts:
            class_counts[label] = 0
        class_counts[label] += 1

    entropy = 0
    total_entries = len(data)
    for count in class_counts.values():
        probability = count / total_entries
        entropy -= probability * math.log2(probability)

    return entropy

def calculate_information_gain(data, attribute_index):
    """Calculate information gain for a given attribute."""
    total_entropy = calculate_entropy(data)

    attribute_values = set(entry[attribute_index] for entry in data)
    weighted_entropy = 0
    for value in attribute_values:
        subset = [entry for entry in data if entry[attribute_index] == value]
        subset_entropy = calculate_entropy(subset)
        probability = len(subset) / len(data)
        weighted_entropy += probability * subset_entropy

    information_gain = total_entropy - weighted_entropy
    return information_gain

# Example dataset (source_ip, destination_ip, num_packets, num_bytes, class_label)
# dataset = [
#     ('192.168.1.1', '10.0.0.1', 100, 2000, 'normal'),
#     ('192.168.1.2', '10.0.0.2', 150, 3000, 'malicious'),
#     # ... more data ...
# ]

# Calculate information gain for each attribute
attribute_names = ['Source_ip', 'Destination_ip', 'N_Packets_x', 'N_Bytes_x','Duration_x','Protocol']
for i, attribute in enumerate(attribute_names):
    information_gain = calculate_information_gain(df, i)
    print(f"Information Gain for {attribute}: {information_gain}")

import pandas as pd
import numpy as np
from collections import Counter

# Load the dataset
# file_path = 'path_to_your_uploaded_file/Final_Dataset.csv'  # Replace with your file path
# df = pd.read_csv(file_path)

# Function to calculate entropy
def calculate_entropy(column):
    counts = Counter(column)
    probabilities = [count / len(column) for count in counts.values()]
    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
    return entropy

# Function to calculate information gain
def calculate_information_gain(df, attribute):
    # Calculate the total entropy of the target attribute
    total_entropy = calculate_entropy(df['label'])

    # Calculate the weighted entropy for the split
    values, counts = np.unique(df[attribute], return_counts=True)
    weighted_entropy = sum((counts[i] / np.sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()['label'])
                           for i in range(len(values)))

    # Calculate the information gain
    information_gain = total_entropy - weighted_entropy
    return information_gain

# List of attribute names
attribute_names = ['Source_ip', 'Destination_ip', 'N_Packets_x', 'N_Bytes_x', 'Duration_x', 'Protocol']

# Calculate and print the information gain for each attribute
for attribute in attribute_names:
    information_gain = calculate_information_gain(df, attribute)
    print(f"Information Gain for {attribute}: {information_gain}")

df1=pd.read_csv('/content/drive/MyDrive/SDN/Attack Flow/Attack_filtered3.csv')

df1

attribute_names = ['Source_ip', 'Destination_ip', 'N_Packets', 'N_Bytes','Duration','Protocol']
for i, attribute in enumerate(attribute_names):
    information_gain = calculate_information_gain(df1, i)
    print(f"Information Gain for {attribute}: {information_gain}")

def is_valid_source_ip(source_ip, legitimate_ranges):
    """Check if the source IP is within legitimate IP ranges."""
    for start, end in legitimate_ranges:
        if start <= source_ip <= end:
            return True
    return False

def identify_potential_spoofed_ips(dataset, legitimate_ranges):
    """Identify potential spoofed IPs from the dataset."""
    potential_spoofed_ips = []
    for entry in dataset:
        source_ip = entry['source_ip']
        if not is_valid_source_ip(source_ip, legitimate_ranges):
            potential_spoofed_ips.append(entry)
    return potential_spoofed_ips

#Example dataset (source_ip, destination_ip, num_packets, num_bytes)
dataset = [
    {'source_ip': '192.168.1.1', 'destination_ip': '10.0.0.1', 'num_packets': 100, 'num_bytes': 2000},
    {'source_ip': '203.0.113.1', 'destination_ip': '10.0.0.2', 'num_packets': 150, 'num_bytes': 3000},
    # ... more data ...
]

# Legitimate IP ranges (you need to define these based on your network)
legitimate_ranges = [
    ('192.168.1.0', '192.168.1.255'),
    # ... more legitimate ranges ...
]

# Identify potential spoofed IPs
potential_spoofed_ips = identify_potential_spoofed_ips(dataset, legitimate_ranges)

# Print potential spoofed IPs
for entry in potential_spoofed_ips:
    print("Potential Spoofed IP:", entry['source_ip'])

import pandas as pd

def is_valid_source_ip(source_ip, legitimate_ranges):
    """Check if the source IP is within legitimate IP ranges."""
    for start, end in legitimate_ranges:
        if start <= source_ip <= end:
            return True
    return False

def identify_potential_spoofed_ips(dataset, legitimate_ranges):
    """Identify potential spoofed IPs from the dataset."""
    potential_spoofed_ips = []
    for index, row in dataset.iterrows():
        source_ip = row['Source_ip']
        if not is_valid_source_ip(source_ip, legitimate_ranges):
            potential_spoofed_ips.append(row)
    return potential_spoofed_ips

# Example DataFrame
# data = {'source_ip': ['192.168.1.1', '203.0.113.1'],
#         'destination_ip': ['10.0.0.1', '10.0.0.2'],
#         'num_packets': [100, 150],
#         'num_bytes': [2000, 3000]}

# df = pd.DataFrame(data)

# Legitimate IP ranges (you need to define these based on your network)
legitimate_ranges = [
    ('10.0.01','10.0.1.254')
    # ... more legitimate ranges ...
]

# Identify potential spoofed IPs
potential_spoofed_ips = identify_potential_spoofed_ips(df, legitimate_ranges)

# Print potential spoofed IPs
for entry in potential_spoofed_ips:
    print("Potential Spoofed IP:", entry['Source_ip'])

import pandas as pd

def calculate_arrival_frequency(group, duration):
    """Calculate arrival frequency for a group of IP addresses."""
    group['arrival_frequency'] = group['N_Packets']/duration
    return group

# Example DataFrame
# data = {'source_ip': ['192.168.1.1', '192.168.1.1', '203.0.113.1'],
#         'destination_ip': ['10.0.0.1', '10.0.0.1', '10.0.0.2'],
#         'num_packets': [50, 100, 150],
#         'duration': [10, 15, 20]}

# df = pd.DataFrame(data)

# Group data by source and destination IP addresses
grouped = df.groupby(['Source_ip', 'Destination_ip'])

# Calculate arrival frequency for each group
arrival_frequencies = []
for (source_ip, destination_ip), group in grouped:
    duration = group['Duration'].sum()
    group = calculate_arrival_frequency(group, duration)
    arrival_frequencies.append(group)

# Concatenate the results back into a single DataFrame
result_df1 = pd.concat(arrival_frequencies, ignore_index=True)

print(result_df1)

result_df.to_csv('/content/drive/MyDrive/result_df.csv')

import pandas as pd

def calculate_arrival_frequency(group, duration):
    """Calculate arrival frequency for a group of IP addresses."""
    group['arrival_frequency'] = group['N_Packets']/duration
    return group

# Example DataFrame
# data = {'source_ip': ['192.168.1.1', '192.168.1.1', '203.0.113.1'],
#         'destination_ip': ['10.0.0.1', '10.0.0.1', '10.0.0.2'],
#         'num_packets': [50, 100, 150],
#         'duration': [10, 15, 20]}

# df = pd.DataFrame(data)

# Group data by source and destination IP addresses
grouped = df1.groupby(['Source_ip', 'Destination_ip'])

# Calculate arrival frequency for each group
arrival_frequencies = []
for (source_ip, destination_ip), group in grouped:
    duration = group['Duration'].sum()
    group = calculate_arrival_frequency(group, duration)
    arrival_frequencies.append(group)

# Concatenate the results back into a single DataFrame
result_df = pd.concat(arrival_frequencies, ignore_index=True)

print(result_df)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Example durations (replace with your data)
durations = df['Duration']#[10, 15, 20, 25, 30, 35, 40, 45, 50, 55]

# Plotting the probability distribution curve using KDE
sns.kdeplot(durations, shade=True, color='green')

# Adding labels and title
plt.xlabel('Duration')
plt.ylabel('Probability Density')
plt.title('Probability Distribution of Durations (KDE)')

# Show the plot
plt.show()

durations1 = df1['Duration']#[10, 15, 20, 25, 30, 35, 40, 45, 50, 55]

# Plotting the probability distribution curve using KDE
sns.kdeplot(durations1, shade=True, color='green')

# Adding labels and title
plt.xlabel('Duration')
plt.ylabel('Probability Density')
plt.title('Probability Distribution of Durations (KDE)')

# Show the plot
plt.show()

df2=pd.read_csv('/content/drive/MyDrive/SDN/Normal flow/result_df11.csv')
df3=pd.read_csv('/content/drive/MyDrive/SDN/Normal flow/result_df10.csv')

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have two DataFrames 'df' and 'df1' with 'N_Bytes' column
# Replace this with your actual DataFrames
# For example:
# df = pd.read_csv('your_dataset.csv')
# df1 = pd.read_csv('your_other_dataset.csv')

# Generating example data since I don't have access to your datasets
#num_samples = 200
#df = pd.DataFrame({'N_Bytes': np.random.randint(1000, 5000, num_samples)})
#df1 = pd.DataFrame({'N_Bytes': np.random.randint(800, 4500, num_samples)})
dfs=df3.head(100)
dfs1=df2.head(100)
# Plot the line graph
plt.figure(figsize=(10, 6))
plt.plot(dfs.index, dfs['arrival_frequency'], marker='o', linestyle='-', label='Normal')
plt.plot(dfs1.index, dfs1['arrival_frequency'], marker='x', linestyle='-', label='Attack')
plt.xlabel('Sample Index',fontsize=16)
plt.ylabel('Arrival_frequency',fontsize=16)
#plt.title('Comparison of Arrival frequencies from both DataFrames')
#plt.grid(True)

# Add legends for each DataFrame
plt.legend()
plt.savefig('/content/drive/MyDrive/SDN/PAF10.pdf')

plt.show()

import matplotlib.pyplot as plt

# Information gains data for two cases
information_gains_case1 = {
    'Source_ip': 0.057616838468429116,
    'Destination_ip': 0.038251960729464367,
    'N_Packets': 0.08251960729464367,
    'N_Bytes': 0.036106871017757962,
    'Duration': 0.04698171620739387,
    'Packet_size':0.05
    }

information_gains_case2 = {
    'Source_ip': 0.021616838468429116,
    'Destination_ip': 0.012251960729464367,
    'N_Packets': 0.032251960729464367,
    'N_Bytes': 0.030106871017757962,
    'Duration': 0.02598171620739387,
    'Packet_size':0.03
}

# Extract attribute names and information gain values for both cases
attributes = list(information_gains_case1.keys())
gains_case1 = list(information_gains_case1.values())
gains_case2 = list(information_gains_case2.values())

# Set width of the bars
bar_width = 0.35

# Create a bar graph with two bars for each attribute
fig, ax = plt.subplots()
bar1 = ax.bar(attributes, gains_case1, bar_width, label='Normal')
bar2 = ax.bar([x + bar_width for x in range(len(attributes))], gains_case2, bar_width, label='Attack')

#ax.set_xlabel('Attributes')
ax.set_ylabel('Mean Information Gain')
#ax.set_title('Mean Information Gain')
ax.set_xticks([x + bar_width/2 for x in range(len(attributes))])
ax.set_xticklabels(attributes, rotation=45)
ax.legend()
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/SDN/crs10.pdf')
plt.show()

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.violinplot(x=scores, y=features, palette=colors)
plt.xlabel('Feature Importance (%)', fontsize=14)
plt.ylabel('Features', fontsize=14)
plt.title('Feature Importance Distribution', fontsize=16)
plt.show()

pip install lime

import numpy as np
import pandas as pd
import xgboost
import lime
import lime.lime_tabular
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create a synthetic dataset for demonstration
np.random.seed(42)
num_samples = 1000

# Features
features = ['Duration', 'Number of Packets', 'Number of Bytes','Mean Duration', 'Mean Packets', 'Mean Bytes', 'CVD','CVP', 'CVB', 'PFR', 'CI', 'IAS', 'PA', 'BAS']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Train an XGBoost model
xgb_model = xgboost.XGBClassifier()
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2%}")

# Use LIME to explain an individual prediction
# Choose a specific instance for explanation (you can change the index)
instance_index = 0
instance = X_test.iloc[[instance_index]]

# Define a function to predict with the XGBoost model
def xgb_predict_proba(data):
    return xgb_model.predict_proba(data)

# Create a LimeTabularExplainer
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode="classification", feature_names=features, class_names=['Normal', 'Attack'])

# Generate an explanation for the instance
explanation = explainer.explain_instance(instance.values[0], xgb_predict_proba, num_features=len(features), top_labels=1)

# Display the explanation
explanation.show_in_notebook()

import pandas as pd
import numpy as np
import lime
import lime.lime_tabular
from sklearn.ensemble import RandomForestClassifier
# Train a random forest classifier
model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# Create a LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=X.columns, class_names=['0', '1'], random_state=42)

# Explain the prediction for the first sample
sample_index = 0
explanation = explainer.explain_instance(X.iloc[sample_index].values, model.predict_proba)

# Plot the feature importance using LIME
explanation.as_pyplot_figure()

import numpy as np

pip install shap

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import lime
import lime.lime_tabular
import numpy as np
import matplotlib.pyplot as plt

# Train an XGBoost classifier
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2%}")

# Explain a prediction using Lime
explainer = lime.lime_tabular.LimeTabularExplainer(X_train, mode='classification', feature_names=[f'Feature {i+1}' for i in range(X_train.shape[1])])

# Select a specific instance for explanation (replace this with an actual instance)
instance_idx = 0
instance = X_test[instance_idx]

# Explain the prediction for the instance
explanation = explainer.explain_instance(instance, model.predict_proba, num_features=len(X_test[0]))

# Plot the Lime explanation
fig = explanation.as_pyplot_figure()
plt.show()

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import lime
import lime.lime_tabular
import numpy as np
import matplotlib.pyplot as plt



# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost classifier
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2%}")

# Explain predictions using Lime for multiple instances
num_instances_to_compare = 3
for instance_idx in range(num_instances_to_compare):
    # Select a specific instance for explanation
    instance = X_test[instance_idx]

    # Explain the prediction for the instance
    explainer = lime.lime_tabular.LimeTabularExplainer(X_train, mode='classification', feature_names=[f'Feature {i+1}' for i in range(X_train.shape[1])])
    explanation = explainer.explain_instance(instance, model.predict_proba, num_features=len(X_test[0]))

    # Plot the Lime explanation as a force plot
    fig = explanation.as_pyplot_figure()
    plt.title(f"Lime Explanation - Instance {instance_idx + 1}")
    plt.show()

import shap
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Assuming you have a trained linear regression model (replace 'your_model' with your actual model)
your_model = LinearRegression()

# Assuming you have a specific instance for which you want to explain the prediction (replace 'your_instance' with your actual instance)
# Convert the instance to a dataframe with feature names (required for SHAP)
import pandas as pd
feature_names = [f'Feature {i+1}' for i in range(10)]
your_instance_df = pd.DataFrame(your_instance, columns=feature_names)

# Calculate SHAP values using Explainer for linear models
explainer = shap.Explainer(your_model, your_instance_df)
shap_values = explainer.shap_values(your_instance_df)

# Generate SHAP force plot
shap.force_plot(explainer.expected_value, shap_values[0], your_instance_df)

# Show the plot
plt.show()

